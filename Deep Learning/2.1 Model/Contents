2.1 Model

*Batch Normalization_ Accelerating Deep Network Training b
*Binarized Neural Networks_ Training Neural Networks with Weights and Activations Constrained to+ 1 orâˆ’1
*Decoupled Neural Interfaces using Synthetic Gradients
*Dropout_ A Simple Way to Prevent Neural Networks from
*Improving neural networks by preventing
*Layer Normalization
*Net2net_ Accelerating learning via knowledge transfer
*Network Morphism
