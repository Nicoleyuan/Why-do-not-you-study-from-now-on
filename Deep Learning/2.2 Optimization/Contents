2.2 Optimization

*Adam_ A method for stochastic optimization
*Deep compression_ Compressing deep neural network with pruning, trained quantization and huffman coding
*Learning to learn by gradient descent by gradient descent
*On the importance of initialization and momentum in deep learning
*SqueezeNet_ AlexNet-level accuracy with 50x fewer parameters and_ 1MB model size
